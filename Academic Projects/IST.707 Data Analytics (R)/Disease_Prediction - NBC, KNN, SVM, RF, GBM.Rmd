---
title: "Disease Prediction with NBC, KNN, Linear SVM, Non-Linear SVM, RF, and GBM"
author: "Fangzhou Yuan"
date: "2020/4/5"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidyverse)
library(corrplot)
library(plyr)
library(ggplot2)
library(gridExtra)
library(caret)
library(klaR)
library(naivebayes)
library(BBmisc)
library(randomForest)
library(gbm)
library(xgboost)
library(kernlab)
library(pROC)
library(clue)
library(class)
library(readr)
```
**Summary**
This assignment uses various machine learning algorithms to classify and predict if a person have a certain type of disease based on some health statistics as predicting variables, such as age, weight, height, blood pressure, etc. The name of the disease or anyother specific symptomatic information is given, so that domain knowledge and nalysis on predicting variables are the only reference for the designer to build and improve the models.

**Introduction**
From EDA, I extracted some useful information from the given dataset and made some assumptions that may help in model building. Then, later in the machine larning models part, I can verify if the results from models are consistent with my assumptions and preliminary analysis. The way I evaluate different models involded in this assignment is by looking at accuracy and kappa values, also, we can also compare the roc and auc of different models. An in detailed comparison and model selection will be expalined in the conclusion section. Lastly, I can apply the models on the test dataset and output the predictions to a file. (Baseline models are not runed here, but demonstrating as a reference)

**Analysis**
```{r}
#read in data
raw_tdf <- read_csv(file = "Disease Prediction Training(1).csv")
#make a NA table
NA_table <- sort(sapply(raw_tdf, function(x) sum(is.na(x))))
sum(NA_table)
#no NA found

#change column names for simplicity
colnames(raw_tdf)[5] <- "HPressure"
colnames(raw_tdf)[6] <- "LPressure"


```

```{r}
#use max & min to see if there exists outliers  
summary(raw_tdf)
#suspicous columns based on units of metadata: weight, hpressure, lpressure

#delete rows with values that do not make sense
#high blood pressure should be in the range of (0,300], delete rows with outliers
tdf <- filter(raw_tdf,HPressure <= 300 & HPressure > 0)
#low blood pressure should be in the range of (0,200]
tdf <- filter(tdf,LPressure <= 200 & LPressure > 0)

```
the threshold can be adjusted to make the range narrower, but need furthur information
about the data or suggestions from medical professionals

use domain knowledge to check data quality 
```{r}
#check if there exists any rows with low pressure > hgih pressure
sum(tdf$HPressure <= tdf$LPressure)
#filter out these rows since it is technically impossible for humans under normal conditions
tdf <- filter(tdf,LPressure < HPressure)
#all data points are collected from adults, by common sense, live adults have minimum
#body weights greater than 20kg
tdf <- filter(tdf,Weight > 20)
#remove duplicate rows if there is any
tdf <- unique(tdf)
```

EDA
```{r}
#Correlation matrix
#convert character columns to dummy variables
tempdf <- tdf
tempdf$Gender <- as.numeric(revalue(tempdf$Gender,c(female = 0, male = 1)))
tempdf$Cholesterol <- as.numeric(revalue(tempdf$Cholesterol, c(normal = 0, high = 1, 'too high' = 2)))
tempdf$Glucose <- as.numeric(revalue(tempdf$Glucose, c(normal = 0, high = 1, 'too high' = 2)))
#create correlation matrix
tcor <- cor(tempdf,method = "pearson")
corrplot(tcor,type = "upper")
```
from the correlation matrix, we can tell that HPressure is highly positively correlated to LPressue, meaning large higher bound (systolic pressure) always
coexist with large low bound (diaslotic pressure). By enumerating the result of disease as 1 for having disease, from the correlation matrix, we can see 
there is no single attribute taht is highly linearly correlated with having/not having the disease. But higher values of HPressure, LPressure, Age, Weight 
Cholesterol, and Glucose lead to hgiher probability of getting the disease. So We will foucus on these columns on further analysis. 

visulizations
```{r}
#we can dirive the Body Mass Index value (BMI) which is a simple and good obesity identifier calculated using height and weight
#BMI formula: BMI = weight(kg)/height^2 (m)
tempdf <- tdf
tempdf$BMI=tempdf$Weight/((tempdf$Height)*0.01)^2
BMI_plot <- ggplot(tempdf)+geom_boxplot(aes(x=Gender, y=BMI, color=as.factor(Disease)),outlier.shape = NA) +
    coord_cartesian(ylim = c(10,50))
BMI_plot

```
From the plot, we can observe that for both male and female the group of people with the disease have higher BMI values than people in the normal BMI range
(20-30). Thus, if weight and hieght attributes do not work as good classifiers in models built, I may consider replacing these two attributes with the BMI,
as it is a more informative way of relating the original data with health status analysis 

scatter plot of high and low pressure
```{r}
pressure_plot <- ggplot(tdf,aes(x=HPressure,y=LPressure))+geom_point(aes(color=as.factor(Disease)))
pressure_plot

```
The scatter plot shows the distribution of the disease by values of HPressure and LPressue. It is obvious that the majority of the top right part with large 
values of both HPressure and LPressure belongs to disease=YES. While the bottome left cornor of the plot which are the data points of small values of HPressure
and LPressure have mostly disease=0. Thus, we can conclude the disease we are analyzing here is associated with high blood pressue problem, In addition our 
conclusion here is consistent with the previous plot, since overweight has consistently been associated with an increase risk for high blood pressure and metabolic
diseases.

barplots of cholesterol vs disease and glucose vs disease
```{r}
chole_plot <- ggplot(tdf)+geom_bar(aes(x=Cholesterol,fill=factor(Disease)),position = "fill")
glu_plot <- ggplot(tdf)+geom_bar(aes(x=Glucose,fill=factor(Disease)),position = "fill")
grid.arrange(chole_plot,glu_plot,ncol=2)

```
According to the plots, data points with higher level of cholesterol have greater chance of getting the disease, and the trend is more noticable in the cholesterol
plot. But in general, these two factor are not very good classifiers because the entropy of the binary distribution of disease stay high in all levels of glucose and 
cholesterol

*NBC Model*
```{r}
#prepare data 
tempdf <- mutate_if(tdf,is.character,as.factor)
#i have tried with using weights & heights vs BMI, and BMI gives a little bit better performance, so I would use BMI for the naive bayes model and delete the weight 
#and height attributes
tempdf$BMI=tempdf$Weight/((tempdf$Height)*0.01)^2
tempdf = tempdf[c(-3,-4)]
cat_cols <- c('Disease')
tempdf[cat_cols] <- lapply(tempdf[cat_cols], as.factor)
set.seed(111)
nb_train_ind <- createDataPartition(tempdf$Disease, p=0.8, list=FALSE)
nbt <- tempdf[nb_train_ind,]
nbtest <- tempdf[-nb_train_ind,]
```

baseline model 
run with default setting
nb_default <- train(Disease ~ ., data = nbt, method = "naive_bayes")
nb_default
accuracy = 0.694, manipulate hyperparameters to improve performance


final model
```{r}
nb_tune <- train(Disease ~ ., data = nbt, method = "naive_bayes",
                 trControl = trainControl(method = "repeatedcv", number = 3, repeats = 3),
                 tuneGrid = expand.grid(laplace = 0:5, usekernel = c(TRUE, FALSE), adjust = seq(1,5, by = 1)))
nb_tune
```
after fine tuning all of the three hyperparameters of the naive bayes model, the model gives an accuracy of 0.698 and kappa =0.378. 

try predict the rest set
```{r}
predict_nbtest <- predict(nb_tune, newdata = nbtest, type = "raw")
confusionMatrix(predict_nbtest,nbtest$Disease)
```

ROC and AUC
```{r}
predict_nbtest_prob <- predict(nb_tune,newdata = nbtest,type = "prob")
nb_roc_curve <- roc(nbtest$Disease,predict_nbtest_prob$`1`)
plot(nb_roc_curve)
auc(nb_roc_curve)

```


*KNN Model*
data preparation
```{r}
#use the same block of code for correlation matrix to convert charater columns to dummy variables
tempdf <- tdf
tempdf$Gender <- as.numeric(revalue(tempdf$Gender,c(female = 0, male = 1)))
tempdf$Cholesterol <- as.numeric(revalue(tempdf$Cholesterol, c(normal = 0, high = 1, 'too high' = 2)))
tempdf$Glucose <- as.numeric(revalue(tempdf$Glucose, c(normal = 0, high = 1, 'too high' = 2)))
#normalize all the predictors for the knn model
tempdf[-12] <- normalize(tempdf[-12], method = "range" , range = c(0,1), margin = 1)
tempdf[cat_cols] <- lapply(tempdf[cat_cols], as.factor)

set.seed(123)
knn_train_ind <- createDataPartition(tempdf$Disease, p=0.8, list=FALSE)
knnt <- tempdf[knn_train_ind,]
knntest <- tempdf[-knn_train_ind,]
```

run with default setting

knn_default <- train(Disease ~., data = knnt, method = "knn")
knn_default
#result: accuracy = 0.683 (k=9)

#tune model
knn_tune <- train(Disease ~., data = knnt, method = "knn",
                  tuneGrid = data.frame(k=seq(1,25)),
                  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3))
plot(knn_tune)

from the plot, I would chose # of neighboors to be 9, which is at the the elbow point of the whole plot. When K<9, the accuracy increases rapidly with number of neighbors, but when k>9, the accuracy increases slowly and sometimes even drop. So considering variance and bias tradeoff, I pick k=9.

over write the model with k=9 to get final model
```{r}
knn_tune <- train(Disease ~., data = knnt, method = "knn",
                  tuneGrid = data.frame(k=seq(5,9)),
                  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3))
#use the model to predict the validation set
predict_knntest <- predict(knn_tune, newdata = knntest)
confusionMatrix(predict_knntest,knntest$Disease)
#the result of predicting the validation data set has an accuracy = 0.703

```
the final model gives an accuracy of 0.70 and kappa of 0.41, which are better than the results using naive bayes

ROC and AUC
```{r}
predict_knntest_prob <- predict(knn_tune,newdata = knntest,type = "prob")
knn_roc_curve <- roc(knntest$Disease,predict_knntest_prob$`1`)
plot(knn_roc_curve)
auc(knn_roc_curve)
```

*Random Forest*
```{r}
#data preprocessing
tempdf <- mutate_if(tdf,is.character,as.factor)
cat_cols <- c('Smoke', 'Alcohol', 'Exercise', 'Disease')
tempdf[cat_cols] <- lapply(tempdf[cat_cols], as.factor)

set.seed(199)
rf_train_ind <- createDataPartition(tempdf$Disease, p=0.8, list=FALSE)
rft <- tempdf[rf_train_ind,]
rftest <- tempdf[-rf_train_ind,]

```

run with default setting
rf_default <- train(Disease ~ ., data = rft, method = "rf",
                    tuneGrid = expand.grid(mtry=(1:5)),
                    trControl = trainControl(method = "repeatedcv", number =4, repeats = 2))
rf_default


with no specification on mtry and ntree, mtry=2 gives an accuracy of 0.733. I will tune the model with finding an appropriate value of mtry first, then hold it
constant to try different values of ntree to get the optimal model. TrainControl is limited in the default setting, because the dataset is fairly large with 
40k+ rows, to reduce running time, I limited number of cross validation and repeats

tune model
find best mtry and ntrees
sqrt(ncol)=sqrt(12)=3.46

tfmodels <- list()
for (ntree in c(50,100,500,1000)){
  set.seed(202)
  rf_tune <- train(Disease ~ ., data = rft, method = "rf",
                  tuneGrid = expand.grid(mtry=2),
                   trControl = trainControl(method = "repeatedcv", number =4, repeats = 1, search = "grid"))
  key <- toString(ntree)
  tfmodels[[key]] <- rf_tune
}
rf_results <- summary(resamples(tfmodels))

unfortunately all the results with different ntree values are the same. It mighe be due to the selection of our ntree value. Larger values of ntree may give more 
insights, but it will also be more time consuming to run. So due to the time limit of the assignment, I would not perform such tunning.

final model
```{r}
rf_tune <- train(Disease ~ ., data = rft, method = "rf",
                    tuneGrid = expand.grid(mtry=2),
                    trControl = trainControl(method = "repeatedcv", number =4, repeats = 3))
rf_tune
#use tuned model to predict the validation data set
predict_rftest <- predict(rf_tune, newdata = rftest)
confusionMatrix(predict_rftest,rftest$Disease)
#accuracy = 0.73

```

check variable importance
```{r}
var_imp_rf <- varImp(rf_tune)
var_imp_rf

```
The variable importance chart generated by the random forest model is generally consistent with the correlation matrix result. But something new and noticable is that the HPressure plays a more important role than the LPressure. Bsides, both of the variable importance and correlation matrix indicate that this disease is more related to health stats such as blood pressure, hight and weight, rather than potential unwholesome habits such as smoking and drinking

ROC and AUC
```{r}
predict_rftest_prob <- predict(rf_tune,newdata = rftest,type = "prob")
rf_roc_curve <- roc(rftest$Disease,predict_rftest_prob$`1`)
plot(rf_roc_curve)
auc(rf_roc_curve)

```

*GBM Model*
data preprocessing is exactly the same with what I did for random forest, so I would directly use tempdf here. For reproducibility, please run data preprocessing in 
random forest section first and then run this part
```{r}
gbmt <- rft
gbmtest <- rftest
```

run with default setting

gbm_default <- train(Disease ~ ., data = gbmt, method = "gbm" )
the default model gives an accuracy of 0.733 with n.trees = 150


I have also tried using XGBoost, which is a lot more time consuming than regular gbm. But the result of the default settings gives very similar accuracy and kappa with
the gbm result. So I will stay with the gbm now and fine tune the model

gbm_tune <- train(Disease ~ ., data = gbmt, method = "gbm",
                  tuneGrid = expand.grid(interaction.depth= 1, n.trees = (1:30)*50, n.minobsinnode = 5, shrinkage = 0.01),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3))
plot(gbm_tune)

from the plot of accuracy vs number of trees, we can see after n.trees reaches 500, the accuracy slowly increases with more number of trees, and the result with 
n.trees = 1500 only improves the accuracy by less than  0.01 comparing to when n.trees = 500. A similar trend can be found in the kappa values as well. So considering 
both the model performance and efficiency, I use n.trees=500 for the final model


gbm_tune <- train(Disease ~ ., data = gbmt, method = "gbm",
                  tuneGrid = expand.grid(interaction.depth= c(1,3,5,7), n.trees = 500, n.minobsinnode = c(5,10,15), shrinkage = 0.01),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 1))
plot(gbm_tune)


after trying with combinations of interaction.depth and n.minobsinnode values, I found in the range of the values I had tried, there is an increasing trend of accuracy and kappa with larger values of interaction.depth and n.minobsinnode, but onl to a limited extend. Therefor, to balance efficiency and performance, I take interaction.depth=5 and n.minobsinnode=5.Then perform the model with these hyperparameters and better train control again to get the final model.

final model
```{r include=FALSE}
gbm_tune <- train(Disease ~ ., data = gbmt, method = "gbm",
                  tuneGrid = expand.grid(interaction.depth= 5, n.trees = 500, n.minobsinnode = 5, shrinkage = 0.01),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5))

#use final model to predict the validation data

```

apply the model on the validation set
```{r}
predict_gbmtest <- predict(gbm_tune, newdata = gbmtest)
confusionMatrix(predict_gbmtest,gbmtest$Disease)
#accuracy = 0.732, kappa = 0.464

```
ROC and AUC
```{r}
predict_gbmtest_prob <- predict(gbm_tune,newdata = gbmtest,type = "prob")
gbm_roc_curve <- roc(gbmtest$Disease,predict_gbmtest_prob$`1`)
plot(gbm_roc_curve)
auc(gbm_roc_curve)

```

*Linear SVM*
the SVMmodels are based on distance, so normalization and converting categorical variables to numerical are needed, which is the same as the data preprocessing for the knn model
so use the block of code for knn preprocessing here
```{r}
#data preprocessing
tempdf <- tdf
tempdf$Gender <- as.numeric(revalue(tempdf$Gender,c(female = 0, male = 1)))
tempdf$Cholesterol <- as.numeric(revalue(tempdf$Cholesterol, c(normal = 0, high = 1, 'too high' = 2)))
tempdf$Glucose <- as.numeric(revalue(tempdf$Glucose, c(normal = 0, high = 1, 'too high' = 2)))
tempdf$Disease <- factor(tempdf$Disease)

set.seed(301)
svm_train_ind <- createDataPartition(tempdf$Disease, p=0.8, list=FALSE)
svmt <- tempdf[svm_train_ind,]
svmtest <- tempdf[-svm_train_ind,]

```

run the model with different values of soft margin, and see what is a good selection of the value, then resample the model by altering the trainControl

set.seed(3011)
lsvm_tune <- train(Disease ~ ., data = svmt, method = "svmLinear",
                   preProcess = c("center","scale"),
                   trControl = trainControl(method = "boot", number = 1),
                   tuneGrid = expand.grid(C = seq(0.5,1,0.1)))
plot(lsvm_tune)

the range 0.5-1 is not where I started, it is a result of trying wider range and narrowing down. According to the plot, taking the cost as 0.9 gives a good model performance without overfitting the data. While keeping numbers of resampling as 1, and altering the soft marin in 0.5-1, the accuracy and kappa does not vary by alot. As you can see in the plot, the accuracy fluctuates in a range less than 0.0001

get the final model with setting c=0.9 and give it more resampling
```{r}
set.seed(301)
lsvm_tune <- train(Disease ~ ., data = svmt, method = "svmLinear",
                   preProcess = c("center","scale"),
                   trControl = trainControl(method = "boot", number = 5),
                   tuneGrid = expand.grid(C = 0.9))
#use final model to predict the validation data
predict_svmtest <- predict(lsvm_tune, newdata = svmtest)
confusionMatrix(predict_svmtest,svmtest$Disease)
#found accuracy = 0.726 with kappa = 0.453
```
The svm method in caret does not produce probability of binary classification, thus, no roc and auc can be generated from the svm models


*Non-Linear SVM (rbf)*
```{r}
#no need for data preprocessing, because the requirement is the same as what's needed for linear SVM

#tune the hyperparameters sigma and C to get final model
rbf_tune <- train(Disease ~ ., data = svmt, method = "svmRadial",
                  preProcess = c("center","scale"),
                  tuneGrid = expand.grid(sigma =0.1, C=0.4),
                  trControl = trainControl(method = "boot", number = 2))
rbf_tune

#use final model to predict the validation data
predict_rbftest <- predict(rbf_tune, newdata = svmtest)
confusionMatrix(predict_rbftest,svmtest$Disease)
#accuracy = 0.734, kappa = 0.469
```
model tunning here is very time consuming. I planed to set sequences for the sigma and soft margin hyperparameters, but non-linear svm is more complicated comparing to other models, and it may take hours to get a result. So alternatively, I lowered the number of resampling and set one hyperparameter fixed while altering the other one with larger steps. I have to admit the hyperparameters values might not be the combination leading to the optimal model.

**Prediction**
```{r}
#use the models to predict the given test data and write the predicted result into a file
#read in test data
testdf <- read_csv(file = "Disease Prediction Testing(1).csv")

#data cleaning 
#check if there is any missing value in the data
test_NA_table <- sort(sapply(testdf, function(x) sum(is.na(x))))
sum(test_NA_table)
#no missing value found

#change column names for simplicity
colnames(testdf)[6] <- "HPressure"
colnames(testdf)[7] <- "LPressure"
```

#use max & min to see if there exists outliers  
```{r}
summary(testdf)
#suspicous columns: LPressure 


#decide to replace outliers with the extreme values I set for the train dataset
testdf$LPressure[testdf$LPressure > 200] <- 200
testdf$LPressure[testdf$LPressure <= 0] <- 1
testdf$HPressure[testdf$HPressure > 300] <- 300
testdf$HPressure[testdf$HPressure <= 0] <- 0
```

check if there exists any rows with low pressure > hgih pressure
```{r}
sum(testdf$HPressure < testdf$LPressure)
#367 cases found
temp_min <- pmin(testdf$HPressure,testdf$LPressure)
testdf$HPressure <- pmax(testdf$HPressure,testdf$LPressure)
testdf$LPressure <- temp_min

```

use NBC to predict
```{r}
#data preprocessing
tempdf <- mutate_if(testdf,is.character,as.factor)
tempdf$BMI=tempdf$Weight/((tempdf$Height)*0.01)^2
tempdf = tempdf[c(-4,-5)]
#predict result
nbc_result <- predict(nb_tune,newdata=tempdf)
```

use KNN to predict
```{r}
#data preprocessing
tempdf <- testdf
tempdf$Gender <- as.numeric(revalue(tempdf$Gender,c(female = 0, male = 1)))
tempdf$Cholesterol <- as.numeric(revalue(tempdf$Cholesterol, c(normal = 0, high = 1, 'too high' = 2)))
tempdf$Glucose <- as.numeric(revalue(tempdf$Glucose, c(normal = 0, high = 1, 'too high' = 2)))
#normalize all the predictors for the knn model
tempdf <- normalize(tempdf, method = "range" , range = c(0,1), margin = 1)
#predict result
knn_result <- predict(knn_tune,newdata=tempdf)
```

use SVM-Linear to predict
```{r}
#svm linear requires normalized numerical predictors, converting categorical variables can be done the same as what I did for the knn model, normalization is done with
#in the model tunning 
tempdf <- testdf
tempdf$Gender <- as.numeric(revalue(tempdf$Gender,c(female = 0, male = 1)))
tempdf$Cholesterol <- as.numeric(revalue(tempdf$Cholesterol, c(normal = 0, high = 1, 'too high' = 2)))
tempdf$Glucose <- as.numeric(revalue(tempdf$Glucose, c(normal = 0, high = 1, 'too high' = 2)))
lsvm_result <- predict(lsvm_tune,newdata=tempdf)

```

use SVM-Nonlinear to predict
```{r}
#again, the required data format is the same as the dataframe provided for SVM_Linear, so I can directly use tempdf here
rbf_result <- predict(rbf_tune,newdata=tempdf)
```

use GBM to predict
```{r}
#preprocess data
tempdf <- mutate_if(testdf,is.character,as.factor)
cat_cols <- c('Smoke','Alcohol','Exercise')
tempdf[cat_cols] <- lapply(tempdf[cat_cols], as.factor)
gbm_result <- predict(gbm_tune, newdata=tempdf)
```

use RF to predict
```{r}
#the data preprocessing step is the same with GBM, so I can directly use tempdf here
rf_result <- predict(rf_tune, newdata=tempdf)

```

create the prediction result dataframe and output to a file
```{r}
prediction <- subset(testdf, select = 1)
prediction['NBC'] <- nbc_result
prediction['KNN'] <- knn_result
prediction['SVM-Linear'] <- lsvm_result
prediction['SVM-RBF'] <- rbf_result
prediction['RF'] <- rf_result
prediction['GBM'] <- gbm_result
prediction <- as.data.frame(prediction)
head(prediction,5)
#output the dataframe to a .csv file
write.csv(prediction,'prediction.csv')

```

**Conclusion**
Based on the model performance on the training dataset, the models perform on a similar level in terms of accuracy, kappa, and area under curve. Strictly speaking, the tree based classification models perform a little bit better than distance based models. It is possibly because outliers in numerical attributes are not treated very well. The cutoffs of high and low pressure are set to be quite generous, and there exists some data points with abnormal combination of hights and weights.

There are also something that I feel can be improved if I have more time. Firstly, regarding data cleaning and preparation, if more medical information can be provided, such as the extrems of adults' BMI range, high blood pressure and low blood pressue range, so that I can narrow the range down for treating the outliers. Also, there are data points with LPressue > HPressure. If furthur information can be given on these unreliable data points, I can certainly have a training dataset with better quality.

I had only tried for the first model built to replace the highest and weights with BMI, which gave a small increase of the accuracy and kappa values.For model hyperparameter tunning, parallel computing can be very helpful, especially for models with higher complxity and require more computing power. Thus, the model performance can be improved with more precise and efficient hyperparameter tunning.