{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    },
    "colab": {
      "name": "IST-718 Fall 2020 Homework 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVCg27B--mnW"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpGEBdMQ-mnY"
      },
      "source": [
        "# IST 718: Big Data Analytics\n",
        "\n",
        "- Professor: Willard Williamson <wewillia@syr.edu>\n",
        "- Faculty Assistant: Vidushi Mishra <vmishr01@syr.edu>\n",
        "- Faculty Assistant: Pranav Kottoli Radhakrishna <pkottoli@syr.edu>\n",
        "## General instructions:\n",
        "\n",
        "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
        "- There could be tests in some cells (i.e., `assert` and `np.testing.` statements). These tests (if present) are used to grade your answers. **However, the professor and FAs could use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
        "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
        "`Kernel`$\\rightarrow$`Restart and Run All`.  All runtime errors will result in a minimum penalty of half off.\n",
        "- Data Bricks is the official class runtime environment so you should test your code on Data Bricks before submission.  If there is a runtime problem in the grading environment, we will try your code on Data Bricks before making a final grading decision.\n",
        "- All plots shall include descriptinve title and axis labels.  Plot legends shall be included where possible.  Unless stated otherwise, plots can be made using any Python plotting package.  It is understood that spark data structures must be converted to something like numpy or pandas prior to making plots.  All required mathematical operations, filtering, selection, etc., required by a homework question shall be performed in spark prior to converting to numpy or pandas.\n",
        "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
        "- Don't add or remove files from your git repo.\n",
        "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
        "- You are free to add additional code cells around the cells marked `your code here`.\n",
        "- We reserve the right to take points off for operations that are extremely inefficient or \"heavy weight\".  This is a big data class and extremely inefficient operations make a big difference when scaling up to large data sets.  For example, the spark dataframe collect() method is a very heavy weight operation and should not be used unless it there is a real need for it.  An example where collect() might be needed is to get ready to make a plot after filtering a spark dataframe.\n",
        "- import * is not allowed because it is considered a very bad coding practice and in some cases can result in a significant delay (which slows down the grading process) in loading imports.  For example, the statement `from sympy import *` is not allowed.  You must import the specific packages that you need.\n",
        "- If you perform databricks specific operations, you MUST protect those operations in an if statement by calling the is_databricks() function provided in all homework assignments.  For example, if you use dbutils (databricks utilities), only run dbutils commands if is_databricks() returns true.  Runtime errors created in the grading environment by not protecting databricks specific commands in an if statement will result in a runtime error points deduction. \n",
        "- The graders reserve the right to deduct points for subjective things we see with your code.  For example, if we ask you to create a pandas data frame to display values from an investigation and you hard code the values, we will take points off for that.  This is only one of many different things we could find in reviewing your code.  In general, write your code like you are submitting it for a code peer review in industry.  \n",
        "- Level of effort is part of our subjective grading.  Oftentimes there is a large disparity between the level of effort between students who are trying learn, and students who are trying to do the minimum possible to check off an assignment requirement.  For example, in cases where we ask for a more open ended investigation, some students put in significant effort and some students do the minimum possible to meet requirements.  In these cases, we may take points off for students who did not put in much effort as compared to students who did put in a lot of effort.  We feel that the students who did a better job deserve a better grade.  We reserve the right to invoke level of effort grading at any time.\n",
        "- Only use spark, spark machine learning, spark data frames, RDD's, and map reduce to solve all problems unless instructed otherwise.\n",
        "- Unless code is provided which reads data files, __you must use the get_training_filename function povided below to read data files.\"  Runtime errors encountered while grading caused by students not using get_training_filename will result in a minimum of half points off for the problem in question.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izUgN-mF-vr9",
        "outputId": "ddc5ce2f-5a4c-42b5-a890-c12d61311910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Need to install pyspark\n",
        "# if pyspark is already installed, will print a message indicating pyspark already isntalled\n",
        "pip install pyspark\n",
        "\n",
        "# Download tweets.csv from github\n",
        "# If the tweets.csv file does not exist in the colab environment\n",
        "if [[ ! -f ./tweets.csv ]]; then \n",
        "   # download tweets.csv file from github and save it in this colab environment instance\n",
        "   wget https://raw.githubusercontent.com/wewilli1/ist718_data/master/tweets.csv   \n",
        "fi\n",
        "\n",
        "# vefify tweets.csv exits in the colab env - should not print an error message\n",
        "ls tweets.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py): started\n",
            "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=cc3c8e00ca48560e68294c9fd07420279a7584f2ed280f4d5ed8fabf4d57ab2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n",
            "tweets.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2020-10-21 03:49:12--  https://raw.githubusercontent.com/wewilli1/ist718_data/master/tweets.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13727793 (13M) [text/plain]\n",
            "Saving to: ‘tweets.csv’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 2.28M 6s\n",
            "    50K .......... .......... .......... .......... ..........  0% 5.21M 4s\n",
            "   100K .......... .......... .......... .......... ..........  1% 14.4M 3s\n",
            "   150K .......... .......... .......... .......... ..........  1% 6.30M 3s\n",
            "   200K .......... .......... .......... .......... ..........  1% 12.0M 2s\n",
            "   250K .......... .......... .......... .......... ..........  2% 8.22M 2s\n",
            "   300K .......... .......... .......... .......... ..........  2% 11.3M 2s\n",
            "   350K .......... .......... .......... .......... ..........  2% 7.47M 2s\n",
            "   400K .......... .......... .......... .......... ..........  3% 17.7M 2s\n",
            "   450K .......... .......... .......... .......... ..........  3% 11.9M 2s\n",
            "   500K .......... .......... .......... .......... ..........  4% 8.89M 2s\n",
            "   550K .......... .......... .......... .......... ..........  4% 17.4M 2s\n",
            "   600K .......... .......... .......... .......... ..........  4% 8.30M 2s\n",
            "   650K .......... .......... .......... .......... ..........  5% 11.3M 2s\n",
            "   700K .......... .......... .......... .......... ..........  5% 15.3M 2s\n",
            "   750K .......... .......... .......... .......... ..........  5% 9.15M 2s\n",
            "   800K .......... .......... .......... .......... ..........  6% 13.6M 1s\n",
            "   850K .......... .......... .......... .......... ..........  6% 24.4M 1s\n",
            "   900K .......... .......... .......... .......... ..........  7% 8.50M 1s\n",
            "   950K .......... .......... .......... .......... ..........  7% 17.7M 1s\n",
            "  1000K .......... .......... .......... .......... ..........  7% 10.7M 1s\n",
            "  1050K .......... .......... .......... .......... ..........  8% 11.1M 1s\n",
            "  1100K .......... .......... .......... .......... ..........  8% 17.6M 1s\n",
            "  1150K .......... .......... .......... .......... ..........  8% 10.4M 1s\n",
            "  1200K .......... .......... .......... .......... ..........  9% 19.4M 1s\n",
            "  1250K .......... .......... .......... .......... ..........  9% 15.2M 1s\n",
            "  1300K .......... .......... .......... .......... .......... 10% 17.2M 1s\n",
            "  1350K .......... .......... .......... .......... .......... 10% 13.4M 1s\n",
            "  1400K .......... .......... .......... .......... .......... 10% 18.4M 1s\n",
            "  1450K .......... .......... .......... .......... .......... 11% 12.0M 1s\n",
            "  1500K .......... .......... .......... .......... .......... 11% 12.8M 1s\n",
            "  1550K .......... .......... .......... .......... .......... 11% 23.3M 1s\n",
            "  1600K .......... .......... .......... .......... .......... 12% 23.4M 1s\n",
            "  1650K .......... .......... .......... .......... .......... 12% 9.63M 1s\n",
            "  1700K .......... .......... .......... .......... .......... 13% 16.8M 1s\n",
            "  1750K .......... .......... .......... .......... .......... 13% 26.4M 1s\n",
            "  1800K .......... .......... .......... .......... .......... 13% 13.4M 1s\n",
            "  1850K .......... .......... .......... .......... .......... 14% 13.5M 1s\n",
            "  1900K .......... .......... .......... .......... .......... 14% 21.1M 1s\n",
            "  1950K .......... .......... .......... .......... .......... 14% 19.9M 1s\n",
            "  2000K .......... .......... .......... .......... .......... 15% 17.8M 1s\n",
            "  2050K .......... .......... .......... .......... .......... 15% 17.3M 1s\n",
            "  2100K .......... .......... .......... .......... .......... 16% 17.3M 1s\n",
            "  2150K .......... .......... .......... .......... .......... 16% 26.1M 1s\n",
            "  2200K .......... .......... .......... .......... .......... 16% 17.8M 1s\n",
            "  2250K .......... .......... .......... .......... .......... 17% 16.0M 1s\n",
            "  2300K .......... .......... .......... .......... .......... 17% 17.3M 1s\n",
            "  2350K .......... .......... .......... .......... .......... 17% 26.0M 1s\n",
            "  2400K .......... .......... .......... .......... .......... 18% 18.9M 1s\n",
            "  2450K .......... .......... .......... .......... .......... 18% 20.6M 1s\n",
            "  2500K .......... .......... .......... .......... .......... 19% 17.9M 1s\n",
            "  2550K .......... .......... .......... .......... .......... 19% 17.5M 1s\n",
            "  2600K .......... .......... .......... .......... .......... 19% 23.1M 1s\n",
            "  2650K .......... .......... .......... .......... .......... 20% 15.3M 1s\n",
            "  2700K .......... .......... .......... .......... .......... 20% 15.9M 1s\n",
            "  2750K .......... .......... .......... .......... .......... 20% 89.1M 1s\n",
            "  2800K .......... .......... .......... .......... .......... 21% 13.8M 1s\n",
            "  2850K .......... .......... .......... .......... .......... 21% 58.5M 1s\n",
            "  2900K .......... .......... .......... .......... .......... 22% 17.7M 1s\n",
            "  2950K .......... .......... .......... .......... .......... 22% 18.7M 1s\n",
            "  3000K .......... .......... .......... .......... .......... 22% 31.2M 1s\n",
            "  3050K .......... .......... .......... .......... .......... 23% 13.8M 1s\n",
            "  3100K .......... .......... .......... .......... .......... 23% 22.0M 1s\n",
            "  3150K .......... .......... .......... .......... .......... 23% 14.7M 1s\n",
            "  3200K .......... .......... .......... .......... .......... 24%  146M 1s\n",
            "  3250K .......... .......... .......... .......... .......... 24% 18.0M 1s\n",
            "  3300K .......... .......... .......... .......... .......... 24% 38.0M 1s\n",
            "  3350K .......... .......... .......... .......... .......... 25% 15.8M 1s\n",
            "  3400K .......... .......... .......... .......... .......... 25%  224M 1s\n",
            "  3450K .......... .......... .......... .......... .......... 26% 15.4M 1s\n",
            "  3500K .......... .......... .......... .......... .......... 26% 18.5M 1s\n",
            "  3550K .......... .......... .......... .......... .......... 26% 32.8M 1s\n",
            "  3600K .......... .......... .......... .......... .......... 27% 19.1M 1s\n",
            "  3650K .......... .......... .......... .......... .......... 27% 44.6M 1s\n",
            "  3700K .......... .......... .......... .......... .......... 27% 17.2M 1s\n",
            "  3750K .......... .......... .......... .......... .......... 28% 49.6M 1s\n",
            "  3800K .......... .......... .......... .......... .......... 28% 23.8M 1s\n",
            "  3850K .......... .......... .......... .......... .......... 29% 12.3M 1s\n",
            "  3900K .......... .......... .......... .......... .......... 29% 58.2M 1s\n",
            "  3950K .......... .......... .......... .......... .......... 29% 22.2M 1s\n",
            "  4000K .......... .......... .......... .......... .......... 30% 33.2M 1s\n",
            "  4050K .......... .......... .......... .......... .......... 30% 31.8M 1s\n",
            "  4100K .......... .......... .......... .......... .......... 30% 12.3M 1s\n",
            "  4150K .......... .......... .......... .......... .......... 31% 41.2M 1s\n",
            "  4200K .......... .......... .......... .......... .......... 31% 44.2M 1s\n",
            "  4250K .......... .......... .......... .......... .......... 32% 19.4M 1s\n",
            "  4300K .......... .......... .......... .......... .......... 32% 23.5M 1s\n",
            "  4350K .......... .......... .......... .......... .......... 32% 50.9M 1s\n",
            "  4400K .......... .......... .......... .......... .......... 33% 16.3M 1s\n",
            "  4450K .......... .......... .......... .......... .......... 33% 57.5M 1s\n",
            "  4500K .......... .......... .......... .......... .......... 33% 25.7M 1s\n",
            "  4550K .......... .......... .......... .......... .......... 34% 47.1M 1s\n",
            "  4600K .......... .......... .......... .......... .......... 34% 23.1M 1s\n",
            "  4650K .......... .......... .......... .......... .......... 35% 27.7M 1s\n",
            "  4700K .......... .......... .......... .......... .......... 35% 23.0M 1s\n",
            "  4750K .......... .......... .......... .......... .......... 35% 32.4M 1s\n",
            "  4800K .......... .......... .......... .......... .......... 36% 46.8M 1s\n",
            "  4850K .......... .......... .......... .......... .......... 36% 26.0M 1s\n",
            "  4900K .......... .......... .......... .......... .......... 36% 48.0M 1s\n",
            "  4950K .......... .......... .......... .......... .......... 37% 18.5M 1s\n",
            "  5000K .......... .......... .......... .......... .......... 37% 19.9M 1s\n",
            "  5050K .......... .......... .......... .......... .......... 38% 39.9M 1s\n",
            "  5100K .......... .......... .......... .......... .......... 38% 59.2M 0s\n",
            "  5150K .......... .......... .......... .......... .......... 38% 30.7M 0s\n",
            "  5200K .......... .......... .......... .......... .......... 39% 25.3M 0s\n",
            "  5250K .......... .......... .......... .......... .......... 39% 22.7M 0s\n",
            "  5300K .......... .......... .......... .......... .......... 39%  120M 0s\n",
            "  5350K .......... .......... .......... .......... .......... 40% 16.8M 0s\n",
            "  5400K .......... .......... .......... .......... .......... 40%  201M 0s\n",
            "  5450K .......... .......... .......... .......... .......... 41% 35.6M 0s\n",
            "  5500K .......... .......... .......... .......... .......... 41% 34.6M 0s\n",
            "  5550K .......... .......... .......... .......... .......... 41% 24.9M 0s\n",
            "  5600K .......... .......... .......... .......... .......... 42%  210M 0s\n",
            "  5650K .......... .......... .......... .......... .......... 42% 20.8M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 42% 16.7M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 43%  155M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 43% 36.6M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 44% 44.4M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 44% 22.3M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 44% 38.3M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 45% 49.0M 0s\n",
            "  6050K .......... .......... .......... .......... .......... 45%  187M 0s\n",
            "  6100K .......... .......... .......... .......... .......... 45% 14.8M 0s\n",
            "  6150K .......... .......... .......... .......... .......... 46% 36.7M 0s\n",
            "  6200K .......... .......... .......... .......... .......... 46% 96.7M 0s\n",
            "  6250K .......... .......... .......... .......... .......... 46% 35.9M 0s\n",
            "  6300K .......... .......... .......... .......... .......... 47% 28.4M 0s\n",
            "  6350K .......... .......... .......... .......... .......... 47% 33.0M 0s\n",
            "  6400K .......... .......... .......... .......... .......... 48% 90.1M 0s\n",
            "  6450K .......... .......... .......... .......... .......... 48% 40.9M 0s\n",
            "  6500K .......... .......... .......... .......... .......... 48% 21.7M 0s\n",
            "  6550K .......... .......... .......... .......... .......... 49% 34.6M 0s\n",
            "  6600K .......... .......... .......... .......... .......... 49% 65.4M 0s\n",
            "  6650K .......... .......... .......... .......... .......... 49% 16.4M 0s\n",
            "  6700K .......... .......... .......... .......... .......... 50% 47.1M 0s\n",
            "  6750K .......... .......... .......... .......... .......... 50%  121M 0s\n",
            "  6800K .......... .......... .......... .......... .......... 51% 84.1M 0s\n",
            "  6850K .......... .......... .......... .......... .......... 51% 33.5M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 51% 28.0M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 52% 43.8M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 52%  106M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 52% 35.1M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 53% 16.0M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 53% 74.5M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 54% 97.1M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 54% 67.1M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 54% 41.2M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 55% 40.6M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 55% 45.7M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 55% 45.6M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 56% 45.1M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 56% 18.6M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 57% 53.0M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 57% 51.2M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 57% 62.6M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 58% 61.0M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 58% 41.9M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 58% 47.8M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 59% 66.0M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 59% 30.7M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 60% 22.2M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 60% 46.3M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 60%  160M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 61% 40.6M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 61% 51.5M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 61% 35.2M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 62% 49.6M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 62%  169M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 63% 94.6M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 63% 29.9M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 63% 21.2M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 64% 80.2M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 64%  121M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 64% 36.8M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 65% 61.2M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 65% 36.3M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 66%  189M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 66% 46.3M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 66% 89.7M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 67% 34.4M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 67% 67.0M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 67% 20.0M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 68%  189M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 68% 31.5M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 68%  160M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 69% 28.4M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 69%  241M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 70% 40.5M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 70%  123M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 70% 49.7M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 71% 17.0M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 71%  194M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 71%  121M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 72% 26.2M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 72%  149M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 73%  200M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 73% 16.5M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 73%  199M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 74%  260M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 74% 64.4M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 74%  214M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 75% 48.3M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 75% 20.6M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 76% 81.8M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 76%  229M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 76% 29.9M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 77%  245M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 77% 31.5M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 77% 45.6M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 78%  122M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 78%  114M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 79% 57.8M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 79% 51.5M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 79% 24.5M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 80% 78.9M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 80% 85.9M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 80% 37.9M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 81% 69.4M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 81% 83.2M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 82% 58.0M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 82% 35.1M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 82% 99.2M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 83% 68.2M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 83%  180M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 83% 54.0M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 84% 23.7M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 84% 56.8M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 85%  137M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 85%  191M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 85% 24.4M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 86%  163M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 86% 76.9M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 86% 33.8M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 87%  184M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 87% 58.5M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 88%  143M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 88% 63.8M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 88% 23.0M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 89%  207M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 89% 63.7M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 89% 94.5M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 90% 27.8M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 90%  126M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 91%  154M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 91% 98.3M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 91% 27.5M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 92% 75.6M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 92%  211M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 92% 77.8M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 93% 66.6M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 93% 25.4M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 93%  200M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 94% 81.1M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 94% 88.3M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 95% 28.4M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 95% 79.9M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 95%  215M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 96%  116M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 96% 56.8M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 96% 64.8M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 97% 66.1M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 97%  167M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 98% 53.9M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 98% 83.7M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 98% 20.0M 0s\n",
            " 13250K .......... .......... .......... .......... .......... 99%  142M 0s\n",
            " 13300K .......... .......... .......... .......... .......... 99%  194M 0s\n",
            " 13350K .......... .......... .......... .......... .......... 99% 62.1M 0s\n",
            " 13400K ......                                                100%  140M=0.5s\n",
            "\n",
            "2020-10-21 03:49:12 (27.2 MB/s) - ‘tweets.csv’ saved [13727793/13727793]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nJDOfKT-2YR"
      },
      "source": [
        "# import statements\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebdn1WZl-8ZD",
        "outputId": "27635d76-41f0-4ab3-ffaf-2c91b8d4785d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# example code to read the downloaded tweets.csv file on colab\n",
        "tweets_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"tweets.csv\")\n",
        "tweets_df.take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(target='4', id='1467822272', date='Mon Apr 06 22:22:45 PDT 2009', flag='NO_QUERY', user='ersle', text='I LOVE @Health4UandPets u guys r the best!! '),\n",
              " Row(target='4', id='1467822273', date='Mon Apr 06 22:22:45 PDT 2009', flag='NO_QUERY', user='becca210', text='im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!'),\n",
              " Row(target='4', id='1467822283', date='Mon Apr 06 22:22:46 PDT 2009', flag='NO_QUERY', user='Wingman29', text='@DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. '),\n",
              " Row(target='4', id='1467822287', date='Mon Apr 06 22:22:46 PDT 2009', flag='NO_QUERY', user='katarinka', text='Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup'),\n",
              " Row(target='4', id='1467822293', date='Mon Apr 06 22:22:46 PDT 2009', flag='NO_QUERY', user='_EmilyYoung', text='@LovesBrooklyn2 he has that effect on everyone ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z20d_Nd6_Ttf"
      },
      "source": [
        "What problems did you have with colab?  Your comments here:\n",
        "There are certainly some issues with the interface and some functions do not work exactly the way I want it to be. But for the purpose of getting the homework done, I definitely prefer colab than databricks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgzFkz0l-mnf"
      },
      "source": [
        "# Sentiment Analysis\n",
        "In this assignment, you will use the tweets.csv file to perform sentiment analysis. The tweets.csv file contains the following columns:\n",
        "- target: the polarity of the tweet (0 = negative, 4 = positive)\n",
        "- ids: The id of the tweet ( 2087)\n",
        "- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "- user: the user that tweeted (robotickilldozr)\n",
        "- text: the text of the tweet (Lyx is cool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGiBBlFq-mng"
      },
      "source": [
        "# Qustion 1: (10 pts)\n",
        "Read tweets.csv into a spark dataframe named `tweets_df`.  Solutions that do not use `get_training_filename` will be heavily penalized.  Drop all columns except target and text.  Transform the target column such that a negative sentiment is equal to 0 and a positive sentiment is equal to 1.  Determine and print the percentage of positive and negative tweets in the dataframe such that it's easy for the graders to find and interpret your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iQIQ39H-mng"
      },
      "source": [
        "# your code here\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "tweets_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"tweets.csv\")\n",
        "columns_to_drop = ['id', 'date', 'flag', 'user']\n",
        "tweets_df = tweets_df.drop(*columns_to_drop)\n",
        "\n",
        "tweets_df = tweets_df.withColumn('target', regexp_replace('target', '4', '1'))\n",
        "tweets_df = tweets_df.withColumn('target', tweets_df['target'].cast(IntegerType()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5RB58L3-mnk",
        "outputId": "66f643c3-b27e-4fd8-f3d8-6b571ac6a199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# grading cell do not modify\n",
        "tweets_pd = tweets_df.toPandas()\n",
        "display(tweets_pd.head())\n",
        "print(tweets_pd.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>im meeting up with one of my besties tonight! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Being sick can be really cheap when it hurts t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target                                               text\n",
              "0       1       I LOVE @Health4UandPets u guys r the best!! \n",
              "1       1  im meeting up with one of my besties tonight! ...\n",
              "2       1  @DaRealSunisaKim Thanks for the Twitter add, S...\n",
              "3       1  Being sick can be really cheap when it hurts t...\n",
              "4       1    @LovesBrooklyn2 he has that effect on everyone "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(100000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz-uxOQr-mnn"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCkOUQAO-mno"
      },
      "source": [
        "# Question 2: (10 pts)\n",
        "Pre-process the data by creating a pipeline named `tweets_pre_proc_pipe`. Your pipeline should tokenize, remove stop words, and do a TF-IDF transformation.  Fit and execute your pipeline, and create a new dataframe named `tweets_pre_proc_df`.  Print the shape of the resulting TF-IDF data such that it's easy for the graders to find and understand as num rows x num words. Based on the shape of the TF-IDF data, would you expect a logistic regression model to overfit?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GArBzB8V-mnp"
      },
      "source": [
        "# your code here\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "import requests\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
        "\n",
        "tokenizer = Tokenizer().setInputCol('text').setOutputCol('words')\n",
        "\n",
        "sw_filter = StopWordsRemover()\\\n",
        "  .setStopWords(stop_words)\\\n",
        "  .setCaseSensitive(False)\\\n",
        "  .setInputCol(\"words\")\\\n",
        "  .setOutputCol(\"filtered\")\n",
        "\n",
        "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
        "  .setInputCol(\"filtered\")\\\n",
        "  .setOutputCol(\"tf\")\n",
        "\n",
        "idf = IDF()\\\n",
        "  .setInputCol('tf')\\\n",
        "  .setOutputCol('tfidf')\n",
        "\n",
        "\n",
        "cv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(tweets_df)\n",
        "tweets_pre_proc_pipe = Pipeline(stages=[cv_pipeline, idf]).fit(tweets_df)\n",
        "tweets_pre_proc_df = tweets_pre_proc_pipe.transform(tweets_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWMjq16YGt3h",
        "outputId": "f6970cb5-1bcc-426a-c827-f61bdd06303a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('number of rows =',tweets_pre_proc_df.count())\n",
        "temp_df = tweets_pre_proc_df.toPandas()\n",
        "print('number of words =',temp_df['tfidf'].iloc[0].size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of rows = 100000\n",
            "number of words = 13693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4MeO587-mns",
        "outputId": "5d3bb642-38a9-4d1d-f78e-f77529b10f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# grading cell do not modify\n",
        "display(tweets_pre_proc_df.toPandas().head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>words</th>\n",
              "      <th>filtered</th>\n",
              "      <th>tf</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
              "      <td>[i, love, @health4uandpets, u, guys, r, the, b...</td>\n",
              "      <td>[love, @health4uandpets, u, guys, r, best!!]</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>im meeting up with one of my besties tonight! ...</td>\n",
              "      <td>[im, meeting, up, with, one, of, my, besties, ...</td>\n",
              "      <td>[im, meeting, besties, tonight!, wait!!, , -, ...</td>\n",
              "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>(1.1265280578718189, 0.0, 0.0, 0.0, 0.0, 3.183...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
              "      <td>[@darealsunisakim, thanks, for, the, twitter, ...</td>\n",
              "      <td>[@darealsunisakim, thanks, twitter, add,, suni...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Being sick can be really cheap when it hurts t...</td>\n",
              "      <td>[being, sick, can, be, really, cheap, when, it...</td>\n",
              "      <td>[sick, really, cheap, hurts, eat, real, food, ...</td>\n",
              "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>(1.1265280578718189, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
              "      <td>[@lovesbrooklyn2, he, has, that, effect, on, e...</td>\n",
              "      <td>[@lovesbrooklyn2, effect]</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  ...                                              tfidf\n",
              "0       1  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "1       1  ...  (1.1265280578718189, 0.0, 0.0, 0.0, 0.0, 3.183...\n",
              "2       1  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "3       1  ...  (1.1265280578718189, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "4       1  ...  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hklLPQg2-mnw"
      },
      "source": [
        "Your explanation here: \n",
        "I think if we build a logistic regression with the tf-idf data, it will not be overfitting, even if it does, it should not be closely related to the shape of the tf-idf data. Beacause we have 13693 features for the model to train on, comparing to the total instances of the data, which is 100000. The variance of a regression model is proportional to the number of features in the training data, if we have more features than the instances of the training data, the model might be overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCoX_G4Q-mnw"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5B2priL-mnx"
      },
      "source": [
        "# Question 3: (10 pts)\n",
        "Since IDF considers a word's frequency across all documents in a corpus, you can use IDF as a form of inference.  Examine the documentation for the spark ML object that you used to create TF-IDF scores and learn how to extract the IDF scores for words in the corpus.  Create a pandas dataframe containing the 5 most important IDF scores named `most_imp_idf`.  Create another pandas dataframe containing the 5 least important IDF scores named `least_imp_idf`.  Each dataframe shall have 2 columns named `word` and `idf_score`.  Explain in words your interpretation of what the IDF scores mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP8AvXdR-mny"
      },
      "source": [
        "# your code here\n",
        "import pandas as pd\n",
        "vocabulary = tweets_pre_proc_pipe.stages[0].stages[-1].vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3ksaM4dS-Wz",
        "outputId": "8ad87c8b-d7f5-48ea-9e9a-5de473446354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "idf_scores = tweets_pre_proc_pipe.stages[-1].idf.values.tolist()\n",
        "\n",
        "max_idf_val = sorted(idf_scores)[-1]\n",
        "indices = [i for i, x in enumerate(idf_scores) if x == max_idf_val]\n",
        "\n",
        "word = []\n",
        "for i in indices:\n",
        "  word.append(vocabulary[i])\n",
        "\n",
        "max_idf = [max_idf_val]*len(word)\n",
        "max_imp_idf = pd.DataFrame(list(zip(word, max_idf)), columns =['word', 'idf_score'])\n",
        "\n",
        "max_imp_idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>idf_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tsk</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>but..</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thks</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>x(</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ve</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2080</th>\n",
              "      <td>pet.</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2081</th>\n",
              "      <td>@snarkattack</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2082</th>\n",
              "      <td>chidambaram</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2083</th>\n",
              "      <td>@milliemagsaysay</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2084</th>\n",
              "      <td>infection.</td>\n",
              "      <td>9.721176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2085 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  word  idf_score\n",
              "0                  tsk   9.721176\n",
              "1                but..   9.721176\n",
              "2                 thks   9.721176\n",
              "3                   x(   9.721176\n",
              "4                   ve   9.721176\n",
              "...                ...        ...\n",
              "2080              pet.   9.721176\n",
              "2081      @snarkattack   9.721176\n",
              "2082       chidambaram   9.721176\n",
              "2083  @milliemagsaysay   9.721176\n",
              "2084        infection.   9.721176\n",
              "\n",
              "[2085 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yksDZcunblMt",
        "outputId": "9eea3514-6273-4d88-b89d-a9a09258b456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "least_5_idf_values = sorted(idf_scores)[:5]\n",
        "\n",
        "least_5_idf_tokens = []\n",
        "for i in least_5_idf_values:\n",
        "  least_5_idf_tokens.append(idf_scores.index(i))\n",
        "\n",
        "word = []\n",
        "for i in least_5_idf_tokens:\n",
        "  word.append(vocabulary[i])\n",
        "\n",
        "least_imp_idf = pd.DataFrame(list(zip(word, least_5_idf_values)), columns =['word', 'idf_score'])\n",
        "\n",
        "least_imp_idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>idf_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>1.126528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>just</td>\n",
              "      <td>2.588812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm</td>\n",
              "      <td>2.645649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>good</td>\n",
              "      <td>3.015945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>like</td>\n",
              "      <td>3.113850</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word  idf_score\n",
              "0         1.126528\n",
              "1  just   2.588812\n",
              "2   i'm   2.645649\n",
              "3  good   3.015945\n",
              "4  like   3.113850"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIrwEhTm-mn5"
      },
      "source": [
        "Your explanation here: \n",
        "The idf values is calculated by total number of documents / number of documents with a specific word in it. Since the totoal number of documents is a constant (in our case, 100000), less frequently appeared words would have larger idf scores. By doing so, when we are calculating the tfidf value using tf*idf, we are discounting those words that appear more frequently in more documents by the idf score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyN-tX8b-mn6"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKOMxN_5-mn6"
      },
      "source": [
        "# Question 4: (10 pts)\n",
        "Create a new recursive pipeline named `lr_pipe` which starts with `tweets_pre_proc_pipe` and adds a logistic regression model using default hyper parameters.  Fit lr_pipe using `tweets_df`.  Score the model using ROC AUC.  Report the resulting AUC such that it is easy for graders to find and interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzBACx1a-mn7",
        "outputId": "ead95164-bceb-46d0-a178-13911a62ac20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# your code here\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "train_df, test_df = tweets_df.randomSplit([0.7, 0.3], seed=0)\n",
        "\n",
        "lr = LogisticRegression().\\\n",
        "    setLabelCol('target').\\\n",
        "    setFeaturesCol('tfidf')\n",
        "\n",
        "lr_pipeline = Pipeline(stages=[tweets_pre_proc_pipe, lr]).fit(train_df)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='target')\n",
        "evaluator.evaluate(lr_pipeline.transform(test_df))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7451840999127445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3QH_THd-mn9"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3LALqj0-mn9"
      },
      "source": [
        "# Question 5: (10 pts)\n",
        "Create 2 pandas dataframes named `lr_pipe_df_neg` and `lr_pipe_df_pos`which contain 2 colunms: `word` and `score`.  Load the 2 dataframes with the top 10 words and logistic regression coefficients that contribute the most to negative and positive sentiments respectively. Analyze the 2 dataframes and describe if the words make sense.  Do the words look like they are really negative and positive?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30809Etb-mn9",
        "outputId": "637b0949-bb2d-41d4-f8c9-232a20547866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# your code here\n",
        "score = lr_pipeline.stages[-1].coefficients.toArray()\n",
        "coeffs_df = pd.DataFrame({'word': tweets_pre_proc_pipe.stages[0].stages[-1].vocabulary, 'score': score})\n",
        "lr_pipe_df_neg = coeffs_df[coeffs_df.score < 0]\n",
        "lr_pipe_df_pos = coeffs_df[coeffs_df.score > 0]\n",
        "\n",
        "lr_pipe_df_neg.sort_values('score').head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13596</th>\n",
              "      <td>hope?</td>\n",
              "      <td>-9.467643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8380</th>\n",
              "      <td>selfish</td>\n",
              "      <td>-6.744463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11065</th>\n",
              "      <td>painfully</td>\n",
              "      <td>-6.258878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13691</th>\n",
              "      <td>@milliemagsaysay</td>\n",
              "      <td>-5.939989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12955</th>\n",
              "      <td>dec</td>\n",
              "      <td>-5.199164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12560</th>\n",
              "      <td>re.</td>\n",
              "      <td>-5.120213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12541</th>\n",
              "      <td>crummy</td>\n",
              "      <td>-4.793909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10360</th>\n",
              "      <td>why'd</td>\n",
              "      <td>-4.768670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12261</th>\n",
              "      <td>saddened</td>\n",
              "      <td>-4.748368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6770</th>\n",
              "      <td>pirates</td>\n",
              "      <td>-4.690658</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   word     score\n",
              "13596             hope? -9.467643\n",
              "8380            selfish -6.744463\n",
              "11065         painfully -6.258878\n",
              "13691  @milliemagsaysay -5.939989\n",
              "12955               dec -5.199164\n",
              "12560               re. -5.120213\n",
              "12541            crummy -4.793909\n",
              "10360             why'd -4.768670\n",
              "12261          saddened -4.748368\n",
              "6770            pirates -4.690658"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI7NDUzX_Kz1",
        "outputId": "5aee9427-9fb5-4c73-def0-b8f290172336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "lr_pipe_df_pos.sort_values('score', ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7474</th>\n",
              "      <td>reunion</td>\n",
              "      <td>6.487646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10570</th>\n",
              "      <td>pizza?</td>\n",
              "      <td>5.991345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9459</th>\n",
              "      <td>lotr</td>\n",
              "      <td>5.710805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9732</th>\n",
              "      <td>tweeps,</td>\n",
              "      <td>5.456466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9780</th>\n",
              "      <td>interactive</td>\n",
              "      <td>5.447157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9930</th>\n",
              "      <td>ride!</td>\n",
              "      <td>5.346156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6462</th>\n",
              "      <td>romantic</td>\n",
              "      <td>5.248715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12170</th>\n",
              "      <td>peek</td>\n",
              "      <td>4.704929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11677</th>\n",
              "      <td>th?</td>\n",
              "      <td>4.628680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7092</th>\n",
              "      <td>hob</td>\n",
              "      <td>4.527096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              word     score\n",
              "7474       reunion  6.487646\n",
              "10570       pizza?  5.991345\n",
              "9459          lotr  5.710805\n",
              "9732       tweeps,  5.456466\n",
              "9780   interactive  5.447157\n",
              "9930         ride!  5.346156\n",
              "6462      romantic  5.248715\n",
              "12170         peek  4.704929\n",
              "11677          th?  4.628680\n",
              "7092           hob  4.527096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "718ISYdi-mn_",
        "outputId": "14ab761c-adbd-4d37-bd20-1ce0b074e349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "source": [
        "# grading cell - do not modify\n",
        "display(lr_pipe_df_neg)\n",
        "display(lr_pipe_df_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>-0.168379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm</td>\n",
              "      <td>-0.023807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>it's</td>\n",
              "      <td>-0.103550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>day</td>\n",
              "      <td>-0.089533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>got</td>\n",
              "      <td>-0.109349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13686</th>\n",
              "      <td>@anistorm</td>\n",
              "      <td>-1.229415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13688</th>\n",
              "      <td>pet.</td>\n",
              "      <td>-1.684006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13690</th>\n",
              "      <td>chidambaram</td>\n",
              "      <td>-0.495094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13691</th>\n",
              "      <td>@milliemagsaysay</td>\n",
              "      <td>-5.939989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13692</th>\n",
              "      <td>infection.</td>\n",
              "      <td>-1.352647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6607 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   word     score\n",
              "0                       -0.168379\n",
              "2                   i'm -0.023807\n",
              "6                  it's -0.103550\n",
              "8                   day -0.089533\n",
              "9                   got -0.109349\n",
              "...                 ...       ...\n",
              "13686         @anistorm -1.229415\n",
              "13688              pet. -1.684006\n",
              "13690       chidambaram -0.495094\n",
              "13691  @milliemagsaysay -5.939989\n",
              "13692        infection. -1.352647\n",
              "\n",
              "[6607 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>just</td>\n",
              "      <td>0.067518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>good</td>\n",
              "      <td>0.418421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>like</td>\n",
              "      <td>0.006771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-</td>\n",
              "      <td>0.170402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>going</td>\n",
              "      <td>0.022431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13682</th>\n",
              "      <td>that:</td>\n",
              "      <td>0.034066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13684</th>\n",
              "      <td>beach..</td>\n",
              "      <td>0.273839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13685</th>\n",
              "      <td>fc</td>\n",
              "      <td>0.049439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13687</th>\n",
              "      <td>matthews</td>\n",
              "      <td>0.014327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13689</th>\n",
              "      <td>@snarkattack</td>\n",
              "      <td>3.536066</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7080 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               word     score\n",
              "1              just  0.067518\n",
              "3              good  0.418421\n",
              "4              like  0.006771\n",
              "5                 -  0.170402\n",
              "7             going  0.022431\n",
              "...             ...       ...\n",
              "13682         that:  0.034066\n",
              "13684       beach..  0.273839\n",
              "13685            fc  0.049439\n",
              "13687      matthews  0.014327\n",
              "13689  @snarkattack  3.536066\n",
              "\n",
              "[7080 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkLrNU4D-moB"
      },
      "source": [
        "Your explanation here:\n",
        "While some of the words can relate to positive or negative sentiments (e.g. \"saddened\", \"painfully\"), the majority of the words in the above dataframes do not look very positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haHRzIUx-moB"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkxuN9Wh-moC"
      },
      "source": [
        "# Question 6a: (5 pts)\n",
        "The goal of this question is to try to improve the score from question 4 using a regularization grid search on a new pipeline named `lr_pipe_1`. lr_pipe_1 is the same as lr_pipe above but we would like you to create a new pipe for grading purposes only.  I'm not sure if it's possible to increase the score or not.  You will be graded on level of effort to increase the score in relation to other students in the class.  All of your grid search code should be inside the `if enable_grid` statement in the cell below.  The enable_grid boolean is set to true in a grading cell above.  If any of the grid search code executes outside of the if statement, you will not get full credit for the question.  We want the ability to turn off the grid search during grading.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CEG-H2UCadV"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0eZPPUAIi9W"
      },
      "source": [
        "enable_grid = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmJtNK9J-moC",
        "outputId": "71ae8f28-8e22-44f8-838a-6f1c251c8485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# your grid search (and only your grid search) code here\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "\n",
        "lr_estimator = Pipeline(stages=[tweets_pre_proc_pipe, lr])\n",
        "\n",
        "if enable_grid:\n",
        "    # your grid search code here\n",
        "    grid = ParamGridBuilder().\\\n",
        "        addGrid(lr.regParam, [0., 0.01, 0.02]).\\\n",
        "        addGrid(lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
        "        build()\n",
        "    \n",
        "    all_models = []\n",
        "    for j in range(len(grid)):\n",
        "      lr_pipe_1 = lr_estimator.fit(train_df, grid[j])\n",
        "      all_models.append(lr_pipe_1)\n",
        "\n",
        "    all_auc = []\n",
        "    for m in all_models:\n",
        "      all_auc.append(evaluator.evaluate(m.transform(test_df)))\n",
        "    \n",
        "    best_model_idx = np.argmax(all_auc)\n",
        "    print(\"best model index =\", best_model_idx)\n",
        "    print(grid[best_model_idx])\n",
        "    print(all_auc[best_model_idx])\n",
        "\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best model index = 4\n",
            "{Param(parent='LogisticRegression_e328183e6ab0', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_e328183e6ab0', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2}\n",
            "0.8195032060417455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr19fUOy-moE"
      },
      "source": [
        "##### Grading feedback cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghCVrR1l-moF"
      },
      "source": [
        "# Question 6b (5 pts)\n",
        "Build a new pipeline named `lr_pipe_2` which uses the optimized model parameters from the grid search in question 6a above (the best model).  Create 2 variables named alpha and lambda and assign to them the best alpha and lambda produced by the grid search by hard coding the values. Fit and transform lr_pipe_2.  Compare AUC scores between lr_pipe_2 with lr_pipe in question 4.  Create a pandas dataframe named `comapre_1_df` which encapsulates the comparison data.  comapre_1_df Shall have 2 columns: `model_name` and `auc_score`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juFFpaJ-moF"
      },
      "source": [
        "# your optimized model code here\n",
        "\n",
        "alpha = 0.2\n",
        "lamb = 0.01\n",
        "\n",
        "# lr_pipe_2 code here\n",
        "\n",
        "lr_2 = LogisticRegression().\\\n",
        "    setLabelCol('target').\\\n",
        "    setFeaturesCol('tfidf').\\\n",
        "    setRegParam(lamb).\\\n",
        "    setMaxIter(100).\\\n",
        "    setElasticNetParam(alpha)\n",
        "\n",
        "lr_pipe_2 = Pipeline(stages=[tweets_pre_proc_pipe, lr_2]).fit(train_df)\n",
        "\n",
        "lr_2_auc = evaluator.evaluate(lr_pipe_2.transform(test_df))\n",
        "lr_auc = evaluator.evaluate(lr_pipeline.transform(test_df))\n",
        "\n",
        "model_name = ['lr_default','lr_opt']\n",
        "auc_score = [lr_auc, lr_2_auc]\n",
        "compare_1_df = pd.DataFrame(list(zip(model_name, auc_score)), columns =['model_name', 'auc_score'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9EUJUhc-moH",
        "outputId": "9979b328-6ff7-4afd-b9e1-edd74ff6be58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# grading cell - do not modify\n",
        "display(compare_1_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>auc_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lr_default</td>\n",
              "      <td>0.745187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lr_opt</td>\n",
              "      <td>0.819501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   model_name  auc_score\n",
              "0  lr_default   0.745187\n",
              "1      lr_opt   0.819501"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY4SWTe--moJ"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJD7NkM--moJ"
      },
      "source": [
        "# Question 7 (10 pts)\n",
        "Perform inference on lr_pipe_2.  Write code to report how many words were eliminated from the best model in question 6b above (if any) as compared to the model in question 4 above.  Make sure your output is easy for the graders to find and interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpuK7i8j-moJ",
        "outputId": "0f45c31e-8357-4a82-987c-5af379aa52a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# your code here\n",
        "\n",
        "lr_2_weights = lr_pipe_2.stages[-1].coefficients.toArray()\n",
        "lr_2_coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': lr_2_weights})\n",
        "\n",
        "num_no_weight_words_2 = len(lr_2_coeffs_df.query('weight == 0.0'))\n",
        "num_no_weight_words_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbvET33rpSH5",
        "outputId": "b69357d7-2c28-4411-d067-ba0afef1c1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_no_weight_words = len(coeffs_df.query('score == 0.0'))\n",
        "\n",
        "print(num_no_weight_words_2-num_no_weight_words, \"more words are eliminated from the optimized model than the default model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9331 more words are eliminated from the optimized model than the default model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLuODpWO-moL"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9__VLLTs-moM"
      },
      "source": [
        "# Question 8 (10 pts)\n",
        "Perform the same inference analysis that you did in question 5 but name the data frames `lr_pipe_df_neg_1` and `lr_pipe_df_pos_1`.  Compare the word importance results with the results in question 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3o1BkGg-moM",
        "outputId": "1a34ca33-c8b9-4e3c-dd94-18617dc13ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# your code here\n",
        "lr_pipe_df_neg_1 = lr_2_coeffs_df[lr_2_coeffs_df.weight < 0]\n",
        "lr_pipe_df_pos_1 = lr_2_coeffs_df[lr_2_coeffs_df.weight > 0]\n",
        "\n",
        "lr_pipe_df_neg_1.sort_values('weight').head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>sad</td>\n",
              "      <td>-0.508234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>poor</td>\n",
              "      <td>-0.376475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>miss</td>\n",
              "      <td>-0.367589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>missing</td>\n",
              "      <td>-0.354671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>wish</td>\n",
              "      <td>-0.350972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>sick</td>\n",
              "      <td>-0.339820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>lonely</td>\n",
              "      <td>-0.336134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>hurts</td>\n",
              "      <td>-0.331137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>sad.</td>\n",
              "      <td>-0.328362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>sucks</td>\n",
              "      <td>-0.320605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        word    weight\n",
              "55       sad -0.508234\n",
              "169     poor -0.376475\n",
              "35      miss -0.367589\n",
              "171  missing -0.354671\n",
              "37      wish -0.350972\n",
              "95      sick -0.339820\n",
              "557   lonely -0.336134\n",
              "300    hurts -0.331137\n",
              "573     sad. -0.328362\n",
              "258    sucks -0.320605"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68mDhSkgIvdM",
        "outputId": "001135de-ed40-4c15-dc78-8bd57662909f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "lr_pipe_df_pos_1.sort_values('weight', ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>thanks</td>\n",
              "      <td>0.337036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>thank</td>\n",
              "      <td>0.296992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>welcome</td>\n",
              "      <td>0.285944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12576</th>\n",
              "      <td>weights</td>\n",
              "      <td>0.283520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10970</th>\n",
              "      <td>owners</td>\n",
              "      <td>0.268473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>good</td>\n",
              "      <td>0.256038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1059</th>\n",
              "      <td>proud</td>\n",
              "      <td>0.242018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7092</th>\n",
              "      <td>hob</td>\n",
              "      <td>0.239991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>happy</td>\n",
              "      <td>0.239530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4788</th>\n",
              "      <td>peaceful</td>\n",
              "      <td>0.234488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           word    weight\n",
              "29       thanks  0.337036\n",
              "69        thank  0.296992\n",
              "230     welcome  0.285944\n",
              "12576   weights  0.283520\n",
              "10970    owners  0.268473\n",
              "3          good  0.256038\n",
              "1059      proud  0.242018\n",
              "7092        hob  0.239991\n",
              "46        happy  0.239530\n",
              "4788   peaceful  0.234488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZxPitJ5-moO",
        "outputId": "74a7e693-05b3-452d-98cf-f928a11f153f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# grading cell - do not modify\n",
        "display(lr_pipe_df_neg_1)\n",
        "display(lr_pipe_df_pos_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>-0.061240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm</td>\n",
              "      <td>-0.007866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>it's</td>\n",
              "      <td>-0.034158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>day</td>\n",
              "      <td>-0.038860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>got</td>\n",
              "      <td>-0.058676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13681</th>\n",
              "      <td>ebay.</td>\n",
              "      <td>-0.059349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13686</th>\n",
              "      <td>@anistorm</td>\n",
              "      <td>-0.063927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13688</th>\n",
              "      <td>pet.</td>\n",
              "      <td>-0.097605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13691</th>\n",
              "      <td>@milliemagsaysay</td>\n",
              "      <td>-0.146155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13692</th>\n",
              "      <td>infection.</td>\n",
              "      <td>-0.026775</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2163 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   word    weight\n",
              "0                       -0.061240\n",
              "2                   i'm -0.007866\n",
              "6                  it's -0.034158\n",
              "8                   day -0.038860\n",
              "9                   got -0.058676\n",
              "...                 ...       ...\n",
              "13681             ebay. -0.059349\n",
              "13686         @anistorm -0.063927\n",
              "13688              pet. -0.097605\n",
              "13691  @milliemagsaysay -0.146155\n",
              "13692        infection. -0.026775\n",
              "\n",
              "[2163 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>just</td>\n",
              "      <td>0.018627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>good</td>\n",
              "      <td>0.256038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-</td>\n",
              "      <td>0.111182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>love</td>\n",
              "      <td>0.228144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>new</td>\n",
              "      <td>0.118655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13662</th>\n",
              "      <td>sequel</td>\n",
              "      <td>0.038540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13663</th>\n",
              "      <td>restaurant.</td>\n",
              "      <td>0.017350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13669</th>\n",
              "      <td>cracker</td>\n",
              "      <td>0.045965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13679</th>\n",
              "      <td>#cloudforce</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13689</th>\n",
              "      <td>@snarkattack</td>\n",
              "      <td>0.090131</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2193 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               word    weight\n",
              "1              just  0.018627\n",
              "3              good  0.256038\n",
              "5                 -  0.111182\n",
              "11             love  0.228144\n",
              "18              new  0.118655\n",
              "...             ...       ...\n",
              "13662        sequel  0.038540\n",
              "13663   restaurant.  0.017350\n",
              "13669       cracker  0.045965\n",
              "13679   #cloudforce  0.014400\n",
              "13689  @snarkattack  0.090131\n",
              "\n",
              "[2193 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEwZQ4Lj-moP"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq2caM9z-moQ"
      },
      "source": [
        "Your explanation here:\n",
        "The most postive and negative words given b the optimized model makes more sense. Most of the words given do agree with what humans intepret as positive or negative words used to express sentiments. It is also worth noticing that the weights of the most positive and negative words are much smaller (to be more precise, closer to 0) than what's given by the default model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWbOzvl4-moQ"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_XHb8og-moR"
      },
      "source": [
        "# Question 9 (10 pts)\n",
        "Create a receiver operating characteristic (ROC) plot for the best model in question 6.  Briefly describe in words the high level steps needed to build a ROC curve as outlined in lecture.  Convince me you understand the high level steps needed to make a ROC curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi5fBs8h-moR",
        "outputId": "9ee79afc-a616-4877-8aad-fadd5e900f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# your code here\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.plot(lr_pipe_2.stages[-1].summary.roc.select('FPR').collect(),\n",
        "         lr_pipe_2.stages[-1].summary.roc.select('TPR').collect())\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVS-sjOR-moT"
      },
      "source": [
        "Your explanation here:\n",
        "The roc curve is plotted using the true positive rates (tpr) v.s. the false positive rate (fpr) for different cutoff points. Both tpr and fpr can be derived from the confusion matrix. After we obtain the tpr and fpr for each cutoff point, we can plot tpr vs fpr and connect the scatter plot with a smooth curve, and that will become our roc curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytPsyhoY-moT"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa-1_EHM-moT"
      },
      "source": [
        "# Question 10 (10 pts)\n",
        "Learn about [precision/recall](https://en.wikipedia.org/wiki/Precision_and_recall) curves. Using the logistic regression summary object contained in the linear regression object within lr_pipe_2, create a precision recall plot. Similar to the `roc` object which is available in the logistic regression summary, there is a `pr` object which can be used to help create a precision / recall curve.  Note that the precision recall curve is built using the same high level methodology as the ROC curve, but using different metrics calculated from the confusion matrix.  If you understand how a ROC curve is built, you understand how a precision / recall curve is built.  Compare and contrast the differences between precision / recall and ROC curves.  What axis is common and what axis is different between the 2 curves?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciJncslI-moU",
        "outputId": "22e66935-25b8-4e49-fac8-5891339e52bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# your code here\n",
        "plt.figure(figsize=(5,5))\n",
        "\n",
        "plt.plot(lr_pipe_2.stages[-1].summary.pr.select('recall').collect(),\n",
        "         lr_pipe_2.stages[-1].summary.pr.select('precision').collect())\n",
        "plt.xlabel('recall')\n",
        "plt.ylabel('precision')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-llXkl-moV"
      },
      "source": [
        "Your explanation here:\n",
        "As stated in the name, the precision/recall curve is calculated by plotting precision v.s. recall for different cutoff points. Precision is calculated as TP/(TP+FP), and Recall is TP/(TP+FN) which is the same as the true positive rate. The PRC is usually used when the data is imbalanced with the majority of data points lie in the negative class. Because PRC does not include true negative in any part of the calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOb_dsxS-moW"
      },
      "source": [
        "##### Grading Feedback Cell"
      ]
    }
  ]
}